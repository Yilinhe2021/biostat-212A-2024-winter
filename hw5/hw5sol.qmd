---
title: "Biostat 212a Homework 5"
subtitle: "Due Mar 16, 2024 @ 11:59PM"
author: "Yilin He and UID: 905789961"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 9.7.1 (10pts)

1. This problem involves hyper-planes in two dimensions.

### (a) 
Sketch the hyperplane $1 + 3X_1 − X_2 = 0$. Indicate the set of points for which $1 + 3X_1 − X_2 > 0$, as well as the set of points for which $1 + 3X_1 − X_2 < 0$.


### (b) 
On the same plot, sketch the hyper-plane $−2 + X_1 + 2X_2 = 0$. Indicate the set of points for which $−2 + X_1 + 2X_2 > 0$, as well as the set of points for which $−2 + X_1 + 2X_2 < 0$.

```{r, fig.width=10, fig.height=5}

plot(0, type="n", xlab='X1', ylab='X2', 
     ylim = c(-10,10),xlim = c(-5,5)) 
abline(1,3,col="red") #Line (a): 1+3X1-X2=0 
abline(1,-0.5,col="black") #Line (b): -2+X1+2X2=0

# Points where Line(a)>0 and Line(a)<0
points(-2,-4,col="blue",pch=19) 
points(-2,-6,col="green",pch=19)
# Points where Line(b)>0 and Line(b)<0

points(2,1,col="green",pch=19) 
points(2,-1,col="blue",pch=19)

legend(3,6,legend=c("Point > 0", "Point < 0", "line a", "line b"),
       col=c("green", "blue", "red", "black"),
       pch=c(19,19,NA,NA),lty=c(NA,NA,1,1))
```

## ISL Exercise 9.7.2 (10pts)

2. We have seen that in p = 2 dimensions, a linear decision boundary takes the form $\beta_0 +\beta_1X_1 + \beta_2X_2 = 0$. We now investigate a non-linear decision boundary.

### (a) 
Sketch the curve $(1 + X_1)^2 + (2 − X_2)^2 = 4$.


### (b) 
On your sketch, indicate the set of points for which $(1 + X_1)^2  + (2 − X_2)^2 > 4$, as well as the set of points for which $(1 + X_1)^2 + (2 − X_2)^2 ≤ 4$.

```{r, fig.width=8, fig.height=5}
library(plotrix)
# Drawing a circle on a plot with a radius of 2 and center at (-1,2).
plot(x=seq(-3,1), y=seq(0,4),type="n", asp = 1, xlab='X1',
     ylab='X2') 
draw.circle(-1,2,2, border = 'purple')
points(-1,2, col='purple', pch=19)
text(-1,2.2,'Center')

# Points outside decision boundary.
points(c(-4,2,-3),c(1,1,3), col="blue",pch=19)

# Points inside decision boundary.
points(c(-1,-2,0),c(1,2,3), col="red",pch=19)

legend(-4,4,legend=c("Point > 0", "Point < 0", "Decision Boundary"),
       col=c("blue", "red", "purple"),
       pch=c(19,19,NA),lty=c(NA,NA,1))

```



### (c) 
Suppose that a classifier assigns an observation to the blue class if $(1 + X_1)^2 + (2 − X_2)^2 > 4$, and to the red class otherwise. To what class is the observation (0, 0) classified? (−1, 1)? (2, 2)? (3, 8)?



**Answer**: (0,0): Blue,(-1,1): Red,(2,2): Blue,(3,8): Blue


### (d) 
Argue that while the decision boundary in (c) is not linear in terms of $X_1$ and $X_2$, it is linear in terms of $X_1$, $X_1^2$, $X_2$, and $X_2^2$.

**Answer**: 

$$
\begin{align*}
(1 + 2X_1 + X_1^2) + (4 - 4X_2 + X^2_2) &= 4 \\
(1 + 2X_1 + X_1^2) + (4 - 4X_2 + X^2_2) - 4 &= 0 \\
1 + 2X_1 - 4X_2 + X_1^2 + X^2_2 &= 0 \\
\end{align*}
$$

The decision boundary is linear in terms of $X_1$, $X_1^2$, $X_2$, and $X_2^2$.


## Support vector machines (SVMs) on the `Carseats` data set (30pts)

Follow the machine learning workflow to train support vector classifier (same as SVM with linear kernel), SVM with polynomial kernel (tune the degree and regularization parameter $C$), and SVM with radial kernel (tune the scale parameter $\gamma$ and regularization parameter $C$) for classifying `Sales<=8` versus `Sales>8`. Use the same seed as in your HW4 for the initial test/train split and compare the final test AUC and accuracy to those methods you tried in HW4.


### SVM with linear kernel

### SVM with polynomial kernel

```{r}
# 2 Heart data
library(GGally)
library(gtsummary)
library(kernlab)
library(tidyverse)
library(tidymodels)
library(ranger)
library(ISLR2)

carseats <- Carseats %>% 
  print(width = Inf)

carseats %>% tbl_summary(by=Urban)
carseats %>% tbl_summary(by=US)
carseats %>% tbl_summary(by=ShelveLoc) 
```

```{r}
# 3 Initial split into test and non-test sets
set.seed(101)
data_split <- initial_split(carseats, 
                            strata = "Sales",
                            prop = 0.75)
data_split

data_other <- training(data_split)
dim(data_other)
data_test <- testing(data_split)
dim(data_test)
```


```{r}
# 4 recipe (R)
svm_recipe <- 
  recipe(Sales ~ ., 
         data = data_other
         ) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_normalize(all_predictors()) %>%
  prep(training = data_other, retain = TRUE)
svm_recipe
```

```{r}
# 5 Model
svm_mod <- 
  svm_poly(
    mode = "classification", 
    cost = tune(), 
    degree = tune()
  ) %>% 
  set_engine("kernlab")
svm_mod
```

```{r}
# 6 workflow (R)
svm_wf <- workflow() %>% 
  add_recipe(svm_recipe) %>%
  add_model(svm_mod)
svm_wf
```

```{r}
# 7 Tuning grid
carseats_grid <- grid_regular(
    trees(range = c(100L, 300L)),    
    mtry(range = c(1L, 5L)), 
    levels = c(3,5))

# 8 Cross-validation (CV)
## Set cross-validation partitions
set.seed(101)
folds <- vfold_cv(carseats_other, v = 5)

## Fit cross-validation
carseats_fit <- 
  carseats_workflow %>% 
  tune_grid(
    resamples = folds,
    grid = carseats_grid,
    metrics = metric_set(roc_auc, accuracy)
  )

## Visualize CV results
carseats_fit %>% 
  collect_metrics() %>% 
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>% 
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(aes(x = trees, y = mean, color = mtry)) + 
  geom_line() +
  labs(title = "ROC AUC by mtry",
       x = "Num. of Trees",
       y = "CV AUC")

```
The graph displays the cross-validated Area Under the Curve (CV AUC) for a model with different numbers of trees. It is evident that an `mtry` of 1 consistently performs worse than the other values, indicating that a very small `mtry` value is not optimal in this scenario. In contrast, the lines for `mtry` values 2 to 5 rise with the number of trees. `mtry` values of 3 and 5 show similar trends and higher CV AUC values.

```{r}
## Show the top 5 models
carseats_fit %>% 
  show_best("roc_auc")

```

```{r}
## Let’s select the best model.
best_carseats <- 
  carseats_fit %>% 
  select_best("roc_auc")
best_carseats
```

```{r}
# 9 Finalize our model
## Final workflow
final_carseats <- 
  carseats_workflow %>% 
  finalize_workflow(
    best_carseats
  )

## fit the whole training set, then predict test 
final_fit <- 
  final_carseats %>% 
  last_fit(carseats_split)

## test metrics
final_fit %>% 
  collect_metrics()
```

### SVM with radial kernel






## Bonus (10pts)

Let
$$
f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = \beta_0 + \beta^T X. 
$$
Then $f(X)=0$ defines a hyperplane in $\mathbb{R}^p$. Show that $f(x)$ is proportional to the signed distance of a point $x$ to the hyperplane $f(X) = 0$. 
