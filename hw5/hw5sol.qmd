---
title: "Biostat 212a Homework 5"
subtitle: "Due Mar 16, 2024 @ 11:59PM"
author: "Yilin He and UID: 905789961"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 9.7.1 (10pts)

1. This problem involves hyper-planes in two dimensions.

### (a) 
Sketch the hyperplane $1 + 3X_1 − X_2 = 0$. Indicate the set of points for which $1 + 3X_1 − X_2 > 0$, as well as the set of points for which $1 + 3X_1 − X_2 < 0$.

**Answer:**

```{r, fig.width=10, fig.height=5}
library(ggplot2)

x <- seq(-10,10)
y <- 1 + 3 * x
data <- data.frame(x,y)

ggplot() + 
  geom_line(aes(x,y)) +
  geom_ribbon(aes(x, ymin = -Inf, ymax = y), fill = "lightblue", alpha = 0.5) +
  geom_ribbon(aes(x, ymin = y, ymax = Inf), fill = "pink", alpha = 0.5) +
  labs(title = "Hyperplane", x = "X", y = "Y") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_text(aes(x = -3, y = 2, label = "1 + 3X_1 − X_2 < 0"), vjust = -1) +
  geom_text(aes(x = 5, y = -10, label = "1 + 3X_1 − X_2 > 0"), vjust = -1) 

```


### (b) 
On the same plot, sketch the hyper-plane $−2 + X_1 + 2X_2 = 0$. Indicate the set of points for which $−2 + X_1 + 2X_2 > 0$, as well as the set of points for which $−2 + X_1 + 2X_2 < 0$.

**Answer:**

```{r, fig.width=10, fig.height=5}
x <- seq(-5,5,lenght.out = 100)
y1 <- 1 + 3 * x
y2 <- (2 - x) / 2
data1 <- data.frame(x, y = y1)
data2 <- data.frame(x, y = y2)

ggplot() + 
  geom_line(data = data1, aes(x,y), color = "blue") +
  geom_ribbon(data = data1, aes(x, ymin = -Inf, ymax = y), fill = "lightblue", alpha = 0.5) +
  geom_ribbon(data = data1, aes(x, ymin = y, ymax = Inf), fill = "pink", alpha = 0.5) +
  
  geom_line(data = data2, aes(x,y), color = "red") +
  geom_ribbon(data = data2, aes(x, ymin = -Inf, ymax = y), fill = "lightgreen", alpha = 0.5) +
  geom_ribbon(data = data2, aes(x, ymin = y, ymax = Inf), fill = "yellow", alpha = 0.5) +
  
  
  labs(title = "Hyperplane and Regions", x = "X", y = "Y") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_text(aes(x = -2, y = -3, label = "1 + 3X_1 − X_2 < 0"), vjust = -1) +
  geom_text(aes(x = -2, y = -10, label = "1 + 3X_1 − X_2 > 0"), vjust = -1) +
  geom_text(aes(x = 2, y = 2, label = "−2 + X_1 + 2X_2 > 0"), vjust = -1) +
  geom_text(aes(x = 2, y = -4, label = "−2 + X_1 + 2X_2 < 0"), vjust = -1) 
```


## ISL Exercise 9.7.2 (10pts)

2. We have seen that in p = 2 dimensions, a linear decision boundary takes the form $\beta_0 +\beta_1X_1 + \beta_2X_2 = 0$. We now investigate a non-linear decision boundary.

### (a) 
Sketch the curve $(1 + X_1)^2 + (2 − X_2)^2 = 4$.

**Answer:**

```{r, fig.width=10, fig.height=10}
x1 <- seq(-3, 1, length.out = 400)

x2_1 <- 2 + sqrt(4 - (1 + x1)^2)
x2_2 <- 2 - sqrt(4 - (1 + x1)^2)

x1_combined <- c(x1, rev(x1))
x2_combined <- c(x2_1, rev(x2_2))

circle <- data.frame(x1 = x1_combined, x2 = x2_combined)

ggplot() + 
  geom_path(data = circle, aes(x = x1 , y = x2), color = "purple") +
  labs(title = "Circle", x = "X1", y = "X2") +
  geom_point(aes(x = -1, y = 2), color = "purple", size = 3) +
  geom_text(aes(x = -1, y = 2.5, label = "Center (-1,2)"), vjust = -1) +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") 

```

### (b) 
On your sketch, indicate the set of points for which $(1 + X_1)^2  + (2 − X_2)^2 > 4$, as well as the set of points for which $(1 + X_1)^2 + (2 − X_2)^2 ≤ 4$.

**Answer:**

```{r, fig.width=10, fig.height=10}
x1 <- seq(-3, 1, length.out = 400)

x2_1 <- 2 + sqrt(4 - (1 + x1)^2)
x2_2 <- 2 - sqrt(4 - (1 + x1)^2)
x1_combined <- c(x1, rev(x1))
x2_combined <- c(x2_1, rev(x2_2))
circle <- data.frame(x1 = x1_combined, x2 = x2_combined)


ggplot() + 
  geom_path(data = circle, aes(x = x1 , y = x2), color = "purple") +
  labs(title = "Circle", x = "X1", y = "X2") +
  geom_point(aes(x = -1, y = 2), color = "purple", size = 3) +
  geom_text(aes(x = -1, y = 2, label = "Center (-1,2)"), vjust = -1) +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_text(aes(x = 0.5, y = 4, label = "(1 + X_1)^2  + (2 − X_2)^2 > 4"), vjust = -1) +
  geom_text(aes(x = -2, y = 3, label = "(1 + X_1)^2  + (2 − X_2)^2 ≤ 4"), vjust = -1)
```



### (c) 
Suppose that a classifier assigns an observation to the blue class if $(1 + X_1)^2 + (2 − X_2)^2 > 4$, and to the red class otherwise. To what class is the observation (0, 0) classified? (−1, 1)? (2, 2)? (3, 8)?

**Answer:**

```{r, fig.width=10, fig.height=10}
obs <- data.frame(x_coord = c(0, -1, 2, 3),
                  y_coord = c(0,  1, 2, 8))

obs$inside <- ((1 + obs$x_coord)^2 + 
               (2 - obs$y_coord)^2) <= 4


x1 <- seq(-3, 5, length.out = 400)
x2 <- seq(-1, 10, length.out = 400)
grid <- expand.grid(x1 = x1, x2 = x2)

grid$inside <- ((1 + grid$x1)^2 + (2 - grid$x2)^2) <= 4

ggplot() +
  geom_tile(data = grid, 
            aes(x = x1, 
                y = x2, 
                fill = inside),
            alpha = .5) +
  geom_path(data = circle,
            aes(x = x1, y = x2)) + 
  geom_point(aes(x = -1, y = 2), color = "purple") +
  geom_text(aes(x = -1, y = 2, label = "Center (-1,2)"), vjust = -1) +
  geom_point(data = obs,
             aes(x = x_coord, 
                 y = y_coord,
                 color = inside),
             size = 2) +
  labs(title = "Decision boundary for non-linearity",
       x = "X1", 
       y = "X2") +
  scale_fill_manual(values = c("lightblue", "pink"),
                    labels = c(">4", "<4"), 
                    name = "Region") +
  scale_color_manual(values = c("blue", "red"), 
                     labels = c("Outside", "Inside"), 
                     name = "Observation") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_text(aes(x = 0.5, y = 4, label = "(1 + X_1)^2  + (2 − X_2)^2 > 4"), vjust = -1) +
  geom_text(aes(x = -2, y = 3, label = "(1 + X_1)^2  + (2 − X_2)^2 ≤ 4"), vjust = -1) +
  coord_fixed() 

```

### (d) 
Argue that while the decision boundary in (c) is not linear in terms of $X_1$ and $X_2$, it is linear in terms of $X_1$, $X_1^2$, $X_2$, and $X_2^2$.

**Answer:**

$$
\begin{align*}
(1 + 2X_1 + X_1^2) + (4 - 4X_2 + X^2_2) &= 4 \\
(1 + 2X_1 + X_1^2) + (4 - 4X_2 + X^2_2) - 4 &= 0 \\
1 + 2X_1 - 4X_2 + X_1^2 + X^2_2 &= 0 \\
\end{align*}
$$

The decision boundary is linear in terms of $X_1$, $X_1^2$, $X_2$, and $X_2^2$.


## Support vector machines (SVMs) on the `Carseats` data set (30pts)

Follow the machine learning workflow to train support vector classifier (same as SVM with linear kernel), SVM with polynomial kernel (tune the degree and regularization parameter $C$), and SVM with radial kernel (tune the scale parameter $\gamma$ and regularization parameter $C$) for classifying `Sales<=8` versus `Sales>8`. Use the same seed as in your HW4 for the initial test/train split and compare the final test AUC and accuracy to those methods you tried in HW4.

```{r setup, include=FALSE}
# 2 Heart data
library(GGally)
library(gtsummary)
library(kernlab)
library(tidyverse)
library(tidymodels)
library(ISLR2)
library(caret)
library(doParallel)
library(vip)
```

```{r}
carseats <- Carseats %>% 
  mutate(Sales = ifelse(Sales > 8, "Low", "High")) %>%
  drop_na() 
carseats
```

```{r}
# 3 Initial split into test and non-test sets
set.seed(101)
data_split <- initial_split(carseats, 
                            strata = "Sales",
                            prop = 0.75)
data_split

data_other <- training(data_split)
dim(data_other)
data_test <- testing(data_split)
dim(data_test)
```


```{r}
# 4 recipe (R)
svm_recipe <- 
  recipe(Sales ~ ., 
         data = data_other
         ) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors())
svm_recipe
```

### SVM with linear kernel
```{r}
# 5 Model
svm_linear_mod <- 
  svm_linear(
    mode = "classification",
    cost = tune()
) %>%
  set_engine("kernlab")
```

```{r}
# 6 workflow (R)
svm_linear_wf <- workflow() %>% 
  add_recipe(svm_recipe) %>%
  add_model(svm_linear_mod)
```

```{r}
# 7 Tuning grid
param_grid1 <- grid_regular(
  cost(range = c(-3, 2)),
  levels = c(5)
  )
```

```{r}
# 8 Cross-validation (CV)
## Set cross-validation partitions
set.seed(101)
folds <- vfold_cv(data_other, v = 5)

## Fit cross-validation
svm_linear_fit <- 
  svm_linear_wf %>% 
  tune_grid(
    resamples = folds,
    grid = param_grid1,
    metrics = metric_set(roc_auc, accuracy)
  )
```

```{r}
## Visualize CV results
svm_linear_fit %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = cost, y = mean)) + 
  geom_line() +
  geom_point() +
  labs(x = "Cost",
       y = "CV AUC") +
  scale_x_log10()
```
The best model is selected based on the highest AUC. 

```{r}
## Let’s select the best model.
best_linear <- svm_linear_fit %>% 
  select_best("roc_auc")

# 9 Finalize our model
## Final workflow
final_linear_wf <- 
  svm_linear_wf %>% 
  finalize_workflow(
    best_linear
  )

## fit the whole training set, then predict test 
final_linear_fit <- 
  final_linear_wf %>% 
  last_fit(data_split)

## test metrics
final_linear_fit %>% 
  collect_metrics()
```


### SVM with polynomial kernel

```{r}
# 5 Model
svm_polynomial_mod <- 
  svm_poly(
    mode = "classification", 
    cost = tune(), 
    degree = tune()
  ) %>% 
  set_engine("kernlab")
```

```{r}
# 6 workflow (R)
svm_polynomial_wf <- workflow() %>%
  add_recipe(svm_recipe) %>%
  add_model(svm_polynomial_mod)
```

```{r}
# 7 Tuning grid
param_grid2 <- grid_regular(
  cost(range = c(-3, 2)),
  degree(range = c(1, 5)),
  levels = c(5)
  )
```

```{r}
# 8 Cross-validation (CV)
## Set cross-validation partitions
set.seed(101)
folds <- vfold_cv(data_other, v = 5)

## Fit cross-validation
svm_polynomial_fit <-
  svm_polynomial_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid2,
    metrics = metric_set(roc_auc, accuracy)
  )
```

```{r}
svm_polynomial_fit %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = degree, y = mean)) + 
  geom_line() +
  geom_point() +
  labs(x = "Degree",
       y = "CV AUC") +
  scale_x_log10()
```

```{r}
## Let’s select the best model.
best_polynomial <- svm_polynomial_fit %>% 
  select_best("roc_auc")

# 9 Finalize our model
## Final workflow
final_polynomial_wf <-
  svm_polynomial_wf %>%
  finalize_workflow(
    best_polynomial
  )

## fit the whole training set, then predict test 
final_polynomial_fit <-
  final_polynomial_wf %>%
  last_fit(data_split)

## test metrics
final_polynomial_fit %>%
  collect_metrics()
```


```{r}

set.seed(101)
split_obj <- initial_split(data = carseats, 
                           prop = 0.7, 
                           strata = Sales)
train <- training(split_obj)
test <- testing(split_obj)


# Create the recipe
recipe(Sales ~ ., data = train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep() -> recipe_obj

# Bake
train <- bake(recipe_obj, new_data=train)
test <- bake(recipe_obj, new_data=test)

final_polynomial_fit %>% 
  pluck(".workflow", 1) %>% 
  extract_fit_parsnip() %>%
  vip(method = "permute", 
      target = "Sales", 
      metric = "accuracy", 
      pred_wrapper = kernlab::predict, train = train)

```
Price is the most important variable, and it has a significant impact on the model compared to other variables. ShelveLoc_Good is the second most important variable, with CompPrice being very close as well. All of these variables are related to the pricing of the items, which correlates with sales.

```{r}
#Create a classification plot 
svm_rbf_spec <- svm_rbf() %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

svm_rbf_fit <- svm_rbf_spec %>% 
  fit(Sales ~., data = train[,c("Price", "Age", "Sales")])

svm_rbf_fit %>% 
  extract_fit_engine() %>% 
  plot()
```

### SVM with radial kernel

```{r}
# 5 Model
svm_radial_mod <-
  svm_rbf(
    mode = "classification",
    cost = tune(),
    rbf_sigma = tune()
  ) %>%
  set_engine("kernlab")
```


```{r}
# 6 workflow (R)
svm_radial_wf <- workflow() %>%
  add_recipe(svm_recipe) %>%
  add_model(svm_radial_mod)
```

```{r}
# 7 Tuning grid
param_grid3 <- grid_regular(
  cost(range = c(-8, 5)),
  rbf_sigma(range = c(-5, -3)),
  levels = c(14, 5)
  )
```

```{r}
# 8 Cross-validation (CV)
## Set cross-validation partitions
set.seed(101)
folds <- vfold_cv(data_other, v = 5)

## Fit cross-validation
svm_radial_fit <-
  svm_radial_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid3,
    metrics = metric_set(roc_auc, accuracy)
  )
```

```{r}
svm_radial_fit %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = cost, y = mean, alpha = rbf_sigma)) + 
  geom_line(aes(group = rbf_sigma)) +
  geom_point() +
  labs(x = "Cost",
       y = "CV AUC") +
  scale_x_log10()
```

The different shades of lines represent various rbf_sigma values, a parameter that determines the model's behavior. The single training example's influence is determined by the value of the cost. The AUC shows the most significant increase with the darkest line, which has a value of .001, as the cost increases.


```{r}
## Let’s select the best model.
best_radial <- svm_radial_fit %>%
  select_best("roc_auc")

# 9 Finalize our model
## Final workflow
final_radial_wf <-
  svm_radial_wf %>%
  finalize_workflow(
    best_radial
  )


## fit the whole training set, then predict test 
final_radial_fit <-
  final_radial_wf %>%
  last_fit(data_split)

## test metrics
final_radial_fit %>%
  collect_metrics()
```

**Conclusion:**

The best model for using kernels is the Support Vector Machine (SVM) with a linear kernel. It works just as well as the SVM with a polynomial kernel because the best setting for the polynomial kernel acts like a linear one. Both types of SVMs were the top performers, with an accuracy rate of 0.885, which was higher than all other models we compared them with from homework 4. The boosting tree and the SVM with a radial kernel also did well, with accuracies of 0.86. However, the random forest and decision tree models didn't do as well as the others.


## Bonus (10pts)

Let
$$
f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = \beta_0 + \beta^T X. 
$$
Then $f(X)=0$ defines a hyperplane in $\mathbb{R}^p$. Show that $f(x)$ is proportional to the signed distance of a point $x$ to the hyperplane $f(X) = 0$. 


**Answer**:
Given the defined by $f(X) = \beta_0 + \beta^T X = 0$ in $\mathbb{R}^p$, the function $f(x)$ evaluats to:
$$
f(x) = \beta_0 + \beta^T x
$$
where $X_0$ is a point on the hyperplane. The normal vector to the hyperplane is $\beta$. The equation of the hyperplane can be rewritten as
$$
\beta_0 = -\beta^T X_0 \\
$$

than plug in $f(x)$, we have
$$
f(x) = \beta_0 + \beta^T x \\
= -\beta^T X_0 + \beta^T x \\
= \beta^T (x - X_0) 
$$
Thus,
$$
\frac{\beta^T (x - X_0)}{\|\beta\|}
= \frac{f(x)}{\|\beta\|} = d \\
$$
where $d$ is the signed distance of a point $x$ to the hyperplane $f(X) = 0$. Therefore, $f(x)$ is proportional to the signed distance of a point $x$ to the hyper-plane $\|\beta\|^{-1}$.

Thus, $f(x)$ is proportional to the signed distance of a point $x$ to the hyperplane $f(X) = 0$.

